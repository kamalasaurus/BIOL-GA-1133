---
title: "Homework1 - Machine Learning"
author: "Kamal"
output:
  pdf_document: default
  html_document: default
---

# Homework 1
## Marlena Kuhn

### Some informative viewing
I highly recommend watching this series by 3blue1brown to get a sense of what matrix operations are from a vector space perspective and how to visualize them: [Essence of Linear Algebra](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab) .  So far, we will cover the first 4 videos in the series, the rest are enrichment ...or review if you've already done statistics.  We might cover topics like convolution and polynomial multiplication when we get to deep learning, which are a bit different from the vector representation of matrices.  The foundation you build here will be helpful then.

### Matrix Multiplication (2.5pts)

You are a linear algebra expert from Youtube university and have been hired by a local space cadet to help them navigate some vector spaces. They have provided you with the following matrices:

```{r}
A <- matrix(c(1, 2, 3, 4, 5, 6), nrow = 2, byrow = TRUE)
B <- matrix(c(7, 8, 9, 10, 11, 12), nrow = 3, byrow = TRUE)
```

1. Compute the matrix product \(C = AB\). What are the dimensions of \(C\)? (0.5pts)

```{r}
#Computing matrix product C and printing it
C<- A%*%B
print(C)

#Determining the number of rows and columns in matrix C
nrow(C)
ncol(C)

#The dimensions of \(C\) are 2 by 2.
```

2. Compute the outer product of the first column of \(A\) with the first row of \(B\). What is the result? (0.5pts)

```{r}
A
B

#Subsetting the first column of A, which should have 2 numbers.
A_first_col<- A[, 1]

#Subsetting the first row of B, which should have 2 numbers.
B_first_row<- B [1 ,]

#Calculating the outer product
outer_firsts<- A_first_col%o%B_first_row

#Printing the results
print(outer_firsts)
```

3. Verify that the matrix product \(C\) can be expressed as a sum of outer products of the columns of \(A\) and the rows of \(B\). (0.5pts)

```{r}
#Calculating outer product for second column of A and second row of B
A_sec_col<- A[, 2]
B_sec_row<- B [2 ,]
outer_secs<- A_sec_col%o%B_sec_row

#Calculating the outer product for the third column of A and the third row of B
A_third_col<- A[, 3]
B_third_row<- B [3 ,]
outer_thirds<- A_third_col%o%B_third_row

#Calculating the sum of (the three) outer products of the A columns and B rows
sum_of_outer<- outer_firsts + outer_secs + outer_thirds

#Checking is C and the sum of the outer products are the same
C== sum_of_outer

#Yes, C can be expressed as the sum of outer products.
```

4. Explain in your own words the relationship between matrix multiplication and outer products. (0.5pts)
A- Matrix multiplication is the sum of outer products, and outer products are multiplications done to a subset of matrices.


5. Implement a function that takes two matrices as input and returns their product using the outer product method. (0.5pts)

```{r}
matrix_multiply_outer <- function(A, B) {
  #The number of outer products that need to be done is the number of columns in
    #... A (or number of rows in B, as they should be the same)
  n<- ncol(A)
  
  #Creating a matrix shell to put the sums into that is number of rows in A by
    #... number of columns in B
  sum_outer<- matrix(0, ncol= ncol(B), nrow= nrow(A))
  
  #Create a loop to calculate the outer products of certain A columns and B rows
    #... and then add them together
  for (i in 1:n){
    #Subset the specific column from A and row from B
    A_col<- A[, i]
    B_row<- B [i ,]
    #Calculate the outer product
    outer<- A_col%o%B_row
    #Summing the outer products to sum_outer and keeping the new sum_outer
      #... values for the next loop
    new_sum<- sum_outer + outer
    sum_outer<- new_sum
  }
  #The desired result is sum_outer after the loop
  return(sum_outer)
}


#Testing the function using the two matrices defined at the start.
test<-
  matrix_multiply_outer(matrix(c(1, 2, 3, 4, 5, 6), nrow = 2, byrow = TRUE),
                        matrix(c(7, 8, 9, 10, 11, 12), nrow = 3, byrow = TRUE))
#Checking that this result is equal to the previously calculated C.
C== test

#As the C from the function matches the C using the operator, my function works
```

### Chick-Weight (7.5pts)
You are a farmer named Bob and your chickens are getting too fat.  Analyze the ChickWeight dataset in R before your chickens get taken away by the ASPCA. Your task is to explore the relationship between diet and weight gain over time in chicks. Perform the following analyses:

Load the ChickWeight dataset and display its structure.  If it's not already included, you can load the attached chick_weight.csv

```{r}
library(ggplot2)
library(ggcorrplot)
library(dplyr)

data("ChickWeight")
str(ChickWeight)
head(ChickWeight)

# If you need to load from a CSV file, uncomment the line below
# ChickWeight <- read.csv("chick_weight.csv")
```

1. Create a summary table showing the average weight of chicks for each diet at each time point. (0.5pts)

```{r}
#In my mind, there are two types of summary tables (one where the rows and
  #... columns are diets and times and the all of the entries are the averages
  #... and one where each row has a diet, a date, and an average).



#This is the entries are averages only table.

#Create a list of the different time points
time_list<- unique(ChickWeight$Time)
print (time_list)

#Create a list of the different diets
diet_list<- unique(ChickWeight$Diet)
print(diet_list)

#Create summary table dataframe with columns named after the time points and
  #... rows names after the diets (and then convert to table)
avg_weight<- data.frame(matrix(0, ncol= length(time_list),
                               nrow= length(diet_list)))
colnames(avg_weight)<- time_list
rownames(avg_weight)<- diet_list

#Loop to populate weight means at each timestep for diet 1
for (i in 1:length(time_list)){
  time<- time_list[i]
  #Subset ChickWeight for entries with Diet 1 only
  diet_1<- ChickWeight[ChickWeight$Diet==1 ,]
  #Subset Diet 1 entries for entries with time equal to place in loop
  diet_1_time_i<- diet_1[diet_1$Time==time ,]
  #Calculate mean and put in avg_weight table
  avg_d1<- mean(diet_1_time_i$weight)
  avg_weight[1, i]<- avg_d1
}

#Loop to populate weight means at each timestep for diet 2
for (i in 1:length(time_list)){
  time<- time_list[i]
  diet_2<- ChickWeight[ChickWeight$Diet==2 ,]
  diet_2_time_i<- diet_2[diet_2$Time==time ,]
  avg_d2<- mean(diet_2_time_i$weight)
  avg_weight[2, i]<- avg_d2
}

#Loop to populate weight means at each timestep for diet 3
for (i in 1:length(time_list)){
  time<- time_list[i]
  diet_3<- ChickWeight[ChickWeight$Diet==3 ,]
  diet_3_time_i<- diet_3[diet_3$Time==time ,]
  avg_d3<- mean(diet_3_time_i$weight)
  avg_weight[3, i]<- avg_d3
}

#Loop to populate weight means at each timestep for diet 4
for (i in 1:length(time_list)){
  time<- time_list[i]
  diet_4<- ChickWeight[ChickWeight$Diet==4 ,]
  diet_4_time_i<- diet_4[diet_4$Time==time ,]
  avg_d4<- mean(diet_4_time_i$weight)
  avg_weight[4, i]<- avg_d4
}

head(avg_weight)



#This is the row has diet, time, and average table.

#Create a data frame and populate each diet number repeated by the times amount
diet_reps<- c(rep(diet_list[1], times= length(time_list)), rep(diet_list[2],
                            times= length(time_list)),rep(diet_list[3],
                            times= length(time_list)), rep(diet_list[4],
                            times= length(time_list)))

d_t_avg<- data_frame(Diet= diet_reps,
                     Time= NA,
                     Average_Weight= NA)

#Populate time values with a loop
for (i in 1:length(time_list)){
  d_t_avg$Time[i]<- time_list[i]
  d_t_avg$Time[(i + length(time_list))]<- time_list[i]
  d_t_avg$Time[(i + 2*length(time_list))]<- time_list[i]
  d_t_avg$Time[(i + 3*length(time_list))]<- time_list[i]
}

#Calculate and populate teh weight averages with a loop
for (i in 1:nrow(d_t_avg)){
  #Set the diet and time for each row as variables
  d<- d_t_avg$Diet[i]
  t<- d_t_avg$Time[i]
  #Subset the origional chick weight data set by the specific diet and time
  d_subset<- ChickWeight[ChickWeight$Diet==d ,]
  d_t_subset<- d_subset[d_subset$Time==t ,]
  #Calculate and populate the averages
  avg<- mean(d_t_subset$weight)
  d_t_avg$Average_Weight[i]<- avg
}

head(d_t_avg)
```

2. Visualize the weight gain over time for each diet using a line plot with error bars representing the standard error of the mean using ggplot. (0.5pts)

```{r}
#Line plot with error bars for average weights for each diet over time

#Calculate the standard error for each diet group (standard deviation/sqrt n)
d_1<- d_t_avg[d_t_avg$Diet==1 ,]
sd_1<- sd(d_1$Average_Weight)
se_1<- sd_1/sqrt(nrow(d_1))

d_2<- d_t_avg[d_t_avg$Diet==2 ,]
sd_2<- sd(d_2$Average_Weight)
se_2<- sd_2/sqrt(nrow(d_2))

d_3<- d_t_avg[d_t_avg$Diet==3 ,]
sd_3<- sd(d_3$Average_Weight)
se_3<- sd_3/sqrt(nrow(d_3))

d_4<- d_t_avg[d_t_avg$Diet==4 ,]
sd_4<- sd(d_4$Average_Weight)
se_4<- sd_4/sqrt(nrow(d_4))

#Plot average weight vs time to examine the change (grouped by diet)
ggplot(data= d_t_avg, aes(x=Time, y=Average_Weight, group_by(Diet),
                          color= Diet))+
  geom_line()+
  geom_point()+
  #Separate error bars representing standard error of each diet group
  geom_errorbar(data= d_t_avg[d_t_avg$Diet==1 ,], aes(ymax= Average_Weight+se_1,
                                                    ymin= Average_Weight-se_1))+
  geom_errorbar(data= d_t_avg[d_t_avg$Diet==2 ,], aes(ymax= Average_Weight+se_2,
                                                    ymin= Average_Weight-se_2))+
  geom_errorbar(data= d_t_avg[d_t_avg$Diet==3 ,], aes(ymax= Average_Weight+se_3,
                                                    ymin= Average_Weight-se_3))+
  geom_errorbar(data= d_t_avg[d_t_avg$Diet==4 ,], aes(ymax= Average_Weight+se_4,
                                                    ymin= Average_Weight-se_4))



#Line plot with error bars for Chick Weight

#Calculate the standard error for each diet (standard error = sd/ sample size)
fsd_1<- sd(ChickWeight[ChickWeight$Diet==1 ,]$weight)
fse_1<- fsd_1/sqrt(nrow(ChickWeight[ChickWeight$Diet==1 ,]))

fsd_2<- sd(ChickWeight[ChickWeight$Diet==2 ,]$weight)
fse_2<- fsd_2/sqrt(nrow(ChickWeight[ChickWeight$Diet==2 ,]))

fsd_3<- sd(ChickWeight[ChickWeight$Diet==3 ,]$weight)
fse_3<- fsd_3/sqrt(nrow(ChickWeight[ChickWeight$Diet==3 ,]))

fsd_4<- sd(ChickWeight[ChickWeight$Diet==4 ,]$weight)
fse_4<- fsd_4/sqrt(nrow(ChickWeight[ChickWeight$Diet==4 ,]))

#Plot weight vs time separated by diet
ggplot(data= ChickWeight, aes(x=Time, y=weight, group_by(Diet), color= Diet))+
  geom_line()+
  geom_point()+
  #Separate error bars representing standard error of each diet group
  geom_errorbar(data= ChickWeight[ChickWeight$Diet==1 ,], 
                aes(ymax= weight+fse_1, ymin= weight-fse_1))+
  geom_errorbar(data= ChickWeight[ChickWeight$Diet==2 ,], 
                aes(ymax= weight+fse_2, ymin= weight-fse_2))+
  geom_errorbar(data= ChickWeight[ChickWeight$Diet==3 ,], 
                aes(ymax= weight+fse_3, ymin= weight-fse_3))+
  geom_errorbar(data= ChickWeight[ChickWeight$Diet==4 ,], 
                aes(ymax= weight+fse_4, ymin= weight-fse_4))


#This looks extremely busy, so I am plotting the weight vs times separately for 
  #... the diets (with colors corresponding to their original colors in the
  #...combined diet graph)
ggplot(data= ChickWeight[ChickWeight$Diet==1 ,], aes(x=Time, y=weight))+
  geom_line(color= 'salmon')+
  geom_point(color= 'salmon')+
  geom_errorbar(data= ChickWeight[ChickWeight$Diet==1 ,],
                aes(ymax= weight+fse_1, ymin= weight-fse_1))+
  ggtitle("Diet 1")

ggplot(data= ChickWeight[ChickWeight$Diet==2 ,], aes(x=Time, y=weight))+
  geom_line(color= 'green')+
  geom_point(color= 'green')+
  geom_errorbar(data= ChickWeight[ChickWeight$Diet==2 ,],
                aes(ymax= weight+fse_2, ymin= weight-fse_2))+
  ggtitle("Diet 2")

ggplot(data= ChickWeight[ChickWeight$Diet==3 ,],
       aes(x=Time, y=weight))+
  geom_line(color= 'lightblue')+
  geom_point(color= 'lightblue')+
  geom_errorbar(data= ChickWeight[ChickWeight$Diet==3 ,],
                aes(ymax= weight+fse_3, ymin= weight-fse_3))+
  ggtitle("Diet 3")

ggplot(data= ChickWeight[ChickWeight$Diet==4 ,],
       aes(x=Time, y=weight))+
  geom_line(color= 'purple')+
  geom_point(color= 'purple')+
  geom_errorbar(data= ChickWeight[ChickWeight$Diet==4 ,],
                aes(ymax= weight+fse_4, ymin= weight-fse_4))+
  ggtitle("Diet 4")
```

3. Use the ggcorrplot package to create a correlation heatmap of the numeric variables in the dataset.  Convert the diet column to dummy variables (columns) and include them in the correlation analysis. (1pts)

```{r}
#Use dummyVars to create dummy variables for diet
require("caret")
dummy <- dummyVars(" ~ .", data = ChickWeight)
chick_dum <- data.frame(predict(dummy, newdata = ChickWeight))
head(chick_dum)
#Subset the dummyVars version to only include weight and time (numeric)
  #... and all of the diets, excluding the chick number variables.
head(chick_dum)
chick_dum_chickless<- chick_dum[, !grepl("Chick", colnames(chick_dum))]
head(chick_dum_chickless)

#Use ggcorplot and the cor function to create heatmap
require(ggcorrplot)
ggcorrplot(cor(chick_dum_chickless))

#Weight appears to be most correlated with time.
```

4.  Calculate a slope coefficient for each diet and time combination using a custom function.  HINT:  use the lm() function inside calculate slope and add 0+ in the front of the independent variables to get slopes for all but no intercepts, otherwise the lm function will drop one of the diet columns to avoid collinearity. (1pts)

```{r}
calculate_slope <- function(data) {
  #Create empty data frame to hold coefficients.
  #Create empty data frame (from matrix) with column names corresponding to the
    #... overall slope names/types of slopes (Time*Diet emcompasses all of them)
    #... that will hold the calculated slopes
  name_sum<- lm(weight~ 0+Time*Diet, data = ChickWeight)
  name_vec<- names(name_sum$coefficients)
  slope_mat<- matrix(NA, 5, length(name_vec)+1)
  slope_mat[1,1]<- "Variables"
  for (i in 1:length(name_vec)) {
    slope_mat[1,i+1]<- name_vec[i]
    }
  colnames(slope_mat)<-slope_mat[1, ]
  slope_mat<- slope_mat[-1, ]
  slope_df<- as.data.frame(slope_mat)
  
  #Add the desired variable combinations in the "Variable" collumn of the df
  slope_df$Variables[1]<- colnames(data)[2]
  slope_df$Variables[2]<- colnames(data)[4]
  slope_df$Variables[3]<- paste(colnames(data)[2], colnames(data)[4], sep= '+')
  slope_df$Variables[4]<- paste(colnames(data)[2], colnames(data)[4], sep= '*')
  
  #Calculate the lm models and slopes using a loop
  for (i in 1:nrow(slope_df)){
    #Combine desired variables with weight to form weight~0+variables
    form<-
      paste(colnames(data)[1], paste(0, slope_df$Variables[i], sep='+'),
            sep= '~')
    model<- lm(as.formula(form), data= data)
    slp<- names(model$coefficients)
    #Populate the slopes in the data frame using another loop
    for (j in 1:length(slp)){
      coef_name<- slp[j]
      slope_df[i, colnames(slope_df) %in% coef_name]<- model$coefficients[j]
    }
  }
  
  #Return slope data frame
  return(slope_df)
}

calculate_slope(ChickWeight)
```

5.  Write a function that calculates residual sum of squares (RSS), and then compare the minimal value to find the optimal slopes for each parameter and parameter combination. (1pts)

```{r}
#Create a function that calculates rss (from model predictions vs data)
calculate_rss <- function(model, data) {
  pred<- predict(model, data)
  residual<- pred-data$weight
  res_sq<- residual^2
  rss<- sum(res_sq)
  return(rss)
}

#Use the function to get the RSS of all four models (from question 4)
time_rss<- calculate_rss(lm(weight~ 0+Time, data= ChickWeight), ChickWeight)
print(time_rss)

diet_rss<- calculate_rss(lm(weight~ 0+Diet, data= ChickWeight), ChickWeight)
print(diet_rss)

time_plus_diet_rss<- calculate_rss(lm(weight~ 0+Time+Diet, data= ChickWeight),
                                   ChickWeight)
print(time_plus_diet_rss)

time_astr_diet_rss<- calculate_rss(lm(weight~ 0+Time*Diet, data= ChickWeight),
                                   ChickWeight)
print(time_astr_diet_rss)

#Find minimum (from vector of the RSSes)
rss_vec<- c(time_rss, diet_rss, time_plus_diet_rss, time_astr_diet_rss)
rss_min<- min(rss_vec)
rss_vec==rss_min
time_astr_diet_rss==rss_min

#The smallest RSS belongs to Time*Diet, therefore, the model containing
  #... weight~0+Time*Diet is the most accurate/best fits the data.
```

# You can loop through different models and calculate RSS for each
# to find the optimal slopes for each parameter and parameter combination.
# You can work through it manually as well
```{r}
#Calculating RSSes of part 4 modles
time_man_rss<- deviance(lm(weight~ 0+Time, data= ChickWeight))
print(time_man_rss)

diet_man_rss<- deviance(lm(weight~ 0+Diet, data= ChickWeight))
print(diet_man_rss)

time_plus_diet_man_rss<- deviance(lm(weight~ 0+Time+Diet, data= ChickWeight))
print(time_plus_diet_rss)

time_astr_diet_man_rss<- deviance(lm(weight~ 0+Time*Diet, data= ChickWeight))
print(time_astr_diet_man_rss)

#Find the minimum RSS
min_man<-
  min(time_man_rss, diet_man_rss, time_plus_diet_rss, time_astr_diet_man_rss)
time_astr_diet_man_rss==min_man
#The Time*Diet model still has the lowest RSS, and is therefore the best model.
```

6.  Use anova to compare the RSS to see if they're significant -- compare the F statistic.  Use the built in anova function.  (0.5pts)

```{r}
#Name time, diet, time+diet, and time*diet modles (to place in anova function)
time<- lm(weight~ 0+Time, data= ChickWeight)
diet<- lm(weight~ 0+Diet, data= ChickWeight)
time_plus_diet<- lm(weight~ 0+Time+Diet, data= ChickWeight)
time_astr_diet<- lm(weight~ 0+Time*Diet, data= ChickWeight)

#Plug models into anova
anova(time, diet, time_plus_diet, time_astr_diet)

#There is no F statistic for the simple Time and Diet models.
#The large F statistic and small p-value for the Time+Diet model means that the
  #... decrease in RSS (from the Diet model) is significant
#The large F statistic and small p-value for the Time*Diet model means that the
  #... decrease in RSS (from the Time+Diet model) is significant.
```

7. Fit a linear model to assess the effect of diet and time on weight.  Use backwards selection to find the best model just against the p-values of the coefficients.  Use the same approach with the 0 + leading the independent variables to ensure all lines are present. (1pts)

```{r}
#Large, overall model using the dummy vars version of Chick Weight to
  #... individually account for all possable variables
mod_1<-
  lm(weight~ 0+ Time+Diet.1+Diet.2+Diet.3+Diet.4+Time:Diet.1+Time:Diet.2+
       Time:Diet.3+Time:Diet.4, data= chick_dum_chickless)
anova(mod_1)
#Diet.2 has the largest P-value and the only one larger than .05

#Same model as above, but without Diet.2
mod_2<- lm(weight~ 0+ Time+Diet.1+Diet.3+Diet.4+Time:Diet.1+Time:Diet.2+
             Time:Diet.3+Time:Diet.4, data= chick_dum_chickless)
anova(mod_2)
#Time:Diet.2 has the largest P-value

#Same model as above, but without Time:Diet.2
mod_3<- lm(weight~ 0+ Time+Diet.1+Diet.3+Diet.4+Time:Diet.1+Time:Diet.3+
             Time:Diet.4, data= chick_dum_chickless)
anova(mod_3)
#Time:Diet.4 has the largest P-value

#Same model as above, but without Time:Diet.4
mod_4<- lm(weight~ 0+ Time+Diet.1+Diet.3+Diet.4+Time:Diet.1+Time:Diet.3,
           data= chick_dum_chickless)
anova(mod_4)
#Time:Diet.3 has the largest P-value and the only one above .05

#Same model as above, but without Time:Diet.3
mod_5<- lm(weight~ 0+ Time+Diet.1+Diet.3+Diet.4+Time:Diet.1,
           data= chick_dum_chickless)
anova(mod_5)
#All p-values (of coefficients) are significant, so backwards selection stops

backwards_mod<- mod_5
print(backwards_mod)
```

8. Iteratively enhance with backwards selection.  When the F statistic becomes insignificant, stop.  Do not use the step function, implement your own F test based backwards selection. (1pts)

```{r}
#Create function
f_test_backward_selection <- function(full_model) {
  #To first enter the "when", m has to be greater than .05, so I am starting
    #... with a place-holder
  m=1
  model<- full_model
  while (m>.05) {
    #Find largest variable with p>.05 (if no p>.05, stopping)
    an<- anova(model)
    m<- max(an$`Pr(>F)`, na.rm = T)
    #Remove that variable
    an[an$`Pr(>F)`==m ,]
    del_var<- row.names(an[an$`Pr(>F)`==m ,])[1]
    fix<- paste(". ~ . -", del_var)
    new<- update(model, as.formula(fix))
    #Continue in this "while" until each variable with a p-stat>.05 is
      #... iteratively removed. This coincides with the F statistic becoming 
      #... insignificant.
    model<- new
  }
  #Return finalized model formed through backwards selection
  return(model)
}


#The full model used in part 7 is mod_1, so I am using it to test my function
back_result<- f_test_backward_selection(mod_1)
print(back_result)

names(back_result$coefficients)==names(backwards_mod$coefficients)
back_result$coefficients==backwards_mod$coefficients
#The coefficient names and slopes of the resulting backwards selection models
  #... from part 7 and 8 are the same. I am taking this to mean that both are
  #... (hopefully) correct.
```


9. Create a quadratic line with just weight vs time (quadratic vs linear) -- calculate RSS with results from quadratic to see if it's better. (0.5pts)

```{r}
#Create quadratic with only Time predictor
time_quad<- lm(weight~ Time+ Time^2, data= ChickWeight)

#Calculate RSS with function made in part 5
t_quad_rss<- calculate_rss(time_quad, ChickWeight)
print(t_quad_rss)

#Compare RSS to linear model with just time
t_quad_rss<time_rss
#The quadratic RSS is lower, so it is a better model than the linear time model.

#Compare RSS to Time:Diet model (because it had the lowest linear model RSS)
t_quad_rss<time_astr_diet_rss
#The quadratic RSS is not lower, so it is a worse model than the Time:Diet model.
```


10.  Generate a null model of chick-weight to hypothetically use for forwards selection. (0.5pts)

```{r}
#The null model is that the slopes for the variables are 0.
#This can be represented by an lm that is all intercept and no variable.
#This is demonstrated below.
#As a control, I am keeping the 0+ phrasing at the beginning.
null_model<- lm(weight~0+1, data=ChickWeight)
null_model
```


### Bonus:  Show that slope of linear regression is pearsons correlation r times the ratio of standard deviations for a simple linear model. (1pts)

You can show it analytically by mashing together the equations for ß_1 and r, or you can show it numerically by simulating some data and fitting a linear model and calculating the correlation coefficient and standard deviations.

The equation for sd(X) in terms of sum of squares is:
$$
s_x = \sqrt{\frac{1}{n-1} \sum_{i=1}^n (X_i - \bar{X})^2}
$$
where \(X\) is the predictor variable, \(n\) is the number of observations, and \(\bar{X}\) is the mean of \(X\).

The equation for sd(Y) in terms of sum of squares is:
$$
s_y = \sqrt{\frac{1}{n-1} \sum_{i=1}^n (Y_i - \bar{Y})^2}
$$
where \(Y\) is the response variable, \(n\) is the number of observations, and \(\bar{Y}\) is the mean of \(Y\).

The equation for cov(X,Y) is:
$$
cov(X, Y) = \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar{X})(Y_i - \bar{Y})
$$
where \(X\) is the predictor variable, \(Y\) is the response variable, \(n\) is the number of observations, and \(\bar{X}\) and \(\bar{Y}\) are the means of \(X\) and \(Y\), respectively.


The equation to calculate pearson's correlation is:
$$
r = \frac{cov(X, Y)}{s_x s_y}
$$
where \(cov(X, Y)\) is the covariance between the predictor variable \(X\) and the response variable \(Y\), \(s_x\) is the standard deviation of \(X\), and \(s_y\) is the standard deviation of \(Y\).

The numerical equation for ß_1 in a simple linear regression is:
$$
\beta_1 = \frac{\sum_{i=1}^n(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n(x_i - \bar{x})^2}
$$


The symbolic representation of the same equation is:
$$
\beta_1 = \frac{cov(X, Y)}{s_x^2}
$$
where \(cov(X, Y)\) is the covariance between the predictor variable \(X\) and the response variable \(Y\), and \(s_x^2\) is the variance of \(X\).


Can you prove the following by apply in algebra with the equations above:
$$
\beta_1 = r \frac{s_y}{s_x}
$$
where \(r\) is Pearson's correlation coefficient, \(s_y\) is the standard deviation of the response variable, and \(s_x\) is the standard deviation of the predictor variable.

Write the solution by hand and upload a photo of your proof; or, if using a numerical comparison, submit the code here:

```{r}
require(magick)
proof<- 
  image_read("/Users/marlenakuhn/Desktop/NYU/masters fall/machine learning/Kuhn-H1-Bonus.HEIC")
plot(proof)
```
